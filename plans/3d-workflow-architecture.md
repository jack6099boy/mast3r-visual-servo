# 3D Visual Servo Workflow æ¶æ§‹è¨­è¨ˆ

## æª”æ¡ˆè¦åŠƒ

### template_system ç›®éŒ„çµæ§‹

```
template_system/
â”œâ”€â”€ __init__.py              # åŒ¯å‡ºæ‰€æœ‰æ¨¡çµ„
â”œâ”€â”€ manager.py               # æ¨¡æ¿ç®¡ç†ï¼ˆç¾æœ‰ï¼‰
â”œâ”€â”€ matcher.py               # 2D åŒ¹é…å™¨ï¼ˆç¾æœ‰ï¼‰
â”œâ”€â”€ matcher_3d.py            # ğŸ†• 3D åŒ¹é…å™¨ï¼ˆæ–°å¢ï¼‰
â”œâ”€â”€ annotator.py             # æ¨™è¨»å·¥å…·ï¼ˆç¾æœ‰ï¼‰
â””â”€â”€ visual_servo.py          # ğŸ†• è¦–è¦ºä¼ºæœæ§åˆ¶å™¨ï¼ˆæ–°å¢ï¼‰
```

### æ–°å¢æª”æ¡ˆèªªæ˜

| æª”æ¡ˆ | ç”¨é€” | ä¾è³´ |
|------|------|------|
| `matcher_3d.py` | ä½¿ç”¨ sparse_global_alignment é€²è¡Œ 3D é»è½‰æ› | mast3r |
| `visual_servo.py` | æ•´åˆ 2D/3D åŒ¹é…çš„è¦–è¦ºä¼ºæœæ§åˆ¶è¿´åœˆ | matcher, matcher_3d |

---

## ä½¿ç”¨å ´æ™¯

```
æ©Ÿæ¢°è‡‚ + æ‰‹æ©Ÿç›¸æ©Ÿ
      â”‚
      â–¼
Template (åƒè€ƒåœ–)                    Target (ç•¶å‰æ‹æ”)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    â”Œâ”€â”€â”€â”        â”‚                 â”‚          â”Œâ”€â”€â”€â” â”‚
â”‚    â”‚ â˜… â”‚ â† P1   â”‚      vs         â”‚          â”‚ â˜… â”‚ â”‚ â† P1' åç§»äº†
â”‚    â””â”€â”€â”€â”˜        â”‚                 â”‚          â””â”€â”€â”€â”˜ â”‚
â”‚   ROI å€åŸŸ      â”‚                 â”‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                                     â”‚
 desired_pixel                        current_pixel
      â”‚                                     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–¼
            è¨ˆç®—ä¿®æ­£æ–¹å‘ + æ­¥ä¼
                     â”‚
                     â–¼
              æ©Ÿæ¢°è‡‚è¿­ä»£èª¿æ•´
                     â”‚
                     â–¼
               æ”¶æ–‚åˆ°ç›®æ¨™ä½ç½®
```

**é™åˆ¶æ¢ä»¶ï¼š**
- âŒ ä¸èƒ½åšæ‰‹çœ¼æ ¡æ­£
- âœ… ç›¸å°æ–¹å‘æ˜¯æ­£ç¢ºçš„
- âœ… é€éè¿­ä»£æ”¶æ–‚

---

## MASt3R æ ¸å¿ƒèƒ½åŠ›æ¢³ç†

### è¼¸å‡ºèªªæ˜

| API | è¼¸å‡º | åº§æ¨™ç³» | ç”¨é€” |
|-----|------|--------|------|
| `inference()` | `pred1['pts3d']` | Camera 1 åº§æ¨™ç³» | View1 æ¯å€‹åƒç´ çš„ 3D ä½ç½® |
| `inference()` | `pred2['pts3d_in_other_view']` | Camera 1 åº§æ¨™ç³» | View2 çš„ 3D é»ï¼Œä½†è¡¨ç¤ºåœ¨ Cam1 åº§æ¨™ç³» |
| `sparse_global_alignment()` | `cam2w[]` | ä¸–ç•Œåº§æ¨™ç³» | ç›¸æ©Ÿå§¿æ…‹ (Camera â†’ World) |
| `sparse_global_alignment()` | `pts3d[]` | ä¸–ç•Œåº§æ¨™ç³» | å„ªåŒ–å¾Œçš„ 3D é» |

### âš ï¸ ç›®å‰ç¨‹å¼ç¢¼çš„å•é¡Œ

åœ¨ `3d_polygon_target_transfer.py` ä¸­ï¼š

```python
# Line 68-79: ç”¨ sparse_ga ç²å– pose
scene = sparse_global_alignment(...)
poses = scene.get_im_poses()  # â† ä¸–ç•Œåº§æ¨™ç³»çš„å§¿æ…‹

# Line 96-98: å»ç”¨ inference ç²å– pts3d
output = inference([tuple(images)], self.model, self.device)
points3D1 = output['pred1']['pts3d']  # â† Camera 1 åº§æ¨™ç³»çš„ 3D é»
```

**å•é¡Œï¼šåº§æ¨™ç³»ä¸ä¸€è‡´ï¼**
- `poses` æ˜¯å¾ sparse_ga å„ªåŒ–å¾—åˆ°çš„ä¸–ç•Œåº§æ¨™ç³»
- `pts3d` æ˜¯å¾ inference å¾—åˆ°çš„ Camera 1 åº§æ¨™ç³»
- å…©è€…çš„å°ºåº¦å’Œåº§æ¨™åŸé»å¯èƒ½ä¸åŒ

---

## æ­£ç¢ºçš„ 3D Workflow è¨­è¨ˆ

### æ–¹æ¡ˆ Aï¼šçµ±ä¸€ä½¿ç”¨ sparse_global_alignmentï¼ˆæ¨è–¦ï¼‰

```mermaid
flowchart TD
    A[Template å½±åƒ + ROI + æ¨™è¨˜é» P1] --> C[sparse_global_alignment]
    B[Target å½±åƒ] --> C
    
    C --> D[scene ç‰©ä»¶]
    
    D --> E[scene.get_im_poses]
    D --> F[scene.get_sparse_pts3d]
    D --> G[scene.get_focals]
    
    E --> H[cam2w_1, cam2w_2]
    F --> I[pts3d_1, pts3d_2]
    G --> J[focal_1, focal_2]
    
    subgraph è¨ˆç®—ä¿®æ­£é‡
        H --> K[ç›¸å°å§¿æ…‹ T_rel = inv_cam2w_2 @ cam2w_1]
        I --> L[æ¨™è¨˜é» 3D ä½ç½® P1_3d]
        L --> M[P1_3d åœ¨ Camera2 åº§æ¨™ç³»çš„ä½ç½®]
        J --> N[æŠ•å½±åˆ° Target åƒç´ åº§æ¨™ P1_prime]
    end
    
    N --> O[pixel_error = P1_prime - desired_pixel]
    K --> P[rotation_error = T_rel çš„æ—‹è½‰éƒ¨åˆ†]
    
    O --> Q[ç¶“é©—æ˜ å°„: delta_pos]
    P --> R[ç¶“é©—æ˜ å°„: delta_rot]
    
    Q --> S[æ©Ÿæ¢°è‡‚ç§»å‹•]
    R --> S
    S --> T[è¿­ä»£ç›´åˆ°æ”¶æ–‚]
```

### æ–¹æ¡ˆ A çš„ç¨‹å¼ç¢¼ä¿®æ­£

```python
class PolygonTargetTransfer3D:
    def compute_polygon_transfer(self, template_path, target_path, polygon_orig, target_point):
        """
        æ­£ç¢ºçš„ 3D è½‰æ›æµç¨‹ - çµ±ä¸€ä½¿ç”¨ sparse_global_alignment
        """
        cache_dir = tempfile.mkdtemp()
        try:
            # ===== Step 1: è¼‰å…¥åœ–ç‰‡ =====
            images = load_images([template_path, target_path], size=518)
            
            # ===== Step 2: å»ºç«‹ pairs =====
            pairs_in = [[
                {"idx": 0, "instance": template_path, "img": images[0]["img"], "true_shape": ...},
                {"idx": 1, "instance": target_path, "img": images[1]["img"], "true_shape": ...}
            ]]
            
            # ===== Step 3: sparse_global_alignment =====
            scene = sparse_global_alignment(
                [template_path, target_path],
                pairs_in,
                cache_dir,
                self.model,
                device=self.device,
                lr1=0.0005, niter1=300,
                lr2=0.00005, niter2=0,  # å¯ä»¥é–‹å•Ÿ niter2 åš refinement
            )
            
            # ===== Step 4: å¾ scene ç²å–ä¸€è‡´çš„æ•¸æ“š =====
            poses = scene.get_im_poses()           # [2, 4, 4] cam-to-world
            focals = scene.get_focals()            # [2]
            pts3d_list = scene.get_sparse_pts3d()  # [pts3d_1, pts3d_2] åœ¨ä¸–ç•Œåº§æ¨™ç³»
            
            # ===== Step 5: æ‰¾å‡ºæ¨™è¨˜é»çš„ 3D ä½ç½® =====
            # å°‡æ¨™è¨˜é»åƒç´ åº§æ¨™è½‰æ›ç‚º pts3d ç´¢å¼•
            scale_x = res_shape[1] / orig_shape[1]
            scale_y = res_shape[0] / orig_shape[0]
            target_pixel_res = (target_point[0] * scale_x, target_point[1] * scale_y)
            
            # å¾ pts3d_1 ä¸­å–å¾—æ¨™è¨˜é»çš„ 3D ä½ç½®
            # æ³¨æ„ï¼špts3d æ˜¯ç¨€ç–çš„ï¼Œéœ€è¦æ‰¾æœ€è¿‘çš„é»æˆ–ä½¿ç”¨æ’å€¼
            P1_3d_world = get_3d_point_at_pixel(pts3d_list[0], target_pixel_res)
            
            # ===== Step 6: è¨ˆç®—ç›¸å°å§¿æ…‹ =====
            cam2w_1 = poses[0]  # Template ç›¸æ©Ÿå§¿æ…‹
            cam2w_2 = poses[1]  # Target ç›¸æ©Ÿå§¿æ…‹
            
            # ç›¸å°å§¿æ…‹: Camera2 åˆ° Camera1
            w2cam_2 = np.linalg.inv(cam2w_2)
            T_rel = w2cam_2 @ cam2w_1  # cam1_to_cam2
            
            # ===== Step 7: å°‡ P1 æŠ•å½±åˆ° Target å½±åƒ =====
            P1_3d_cam2 = (w2cam_2 @ np.append(P1_3d_world, 1.0))[:3]
            
            focal2 = float(focals[1])
            cx2, cy2 = res_shape[1] / 2, res_shape[0] / 2
            
            P1_prime_x = focal2 * (P1_3d_cam2[0] / P1_3d_cam2[2]) + cx2
            P1_prime_y = focal2 * (P1_3d_cam2[1] / P1_3d_cam2[2]) + cy2
            
            # è½‰å›åŸå§‹åœ–ç‰‡åº§æ¨™
            P1_prime = (P1_prime_x / scale_x, P1_prime_y / scale_y)
            
            return {
                'P1_prime': P1_prime,              # æ¨™è¨˜é»åœ¨ Target çš„åƒç´ ä½ç½®
                'T_rel': T_rel,                    # ç›¸å°å§¿æ…‹çŸ©é™£
                'P1_3d_world': P1_3d_world,        # æ¨™è¨˜é»çš„ 3D ä¸–ç•Œåº§æ¨™
                'focals': (float(focals[0]), float(focals[1])),
            }
            
        finally:
            shutil.rmtree(cache_dir, ignore_errors=True)
```

---

### æ–¹æ¡ˆ Bï¼šåªä½¿ç”¨ inferenceï¼ˆå¿«é€Ÿç‰ˆï¼‰

å¦‚æœéœ€è¦å¿«é€Ÿæ¨ç†ï¼Œå¯ä»¥åªç”¨ `inference()`ï¼Œä½†éœ€è¦é¡å¤–æ­¥é©Ÿä¼°è¨ˆç›¸å°å§¿æ…‹ï¼š

```mermaid
flowchart TD
    A[Template + Target] --> B[inference]
    B --> C[pred1, pred2]
    
    C --> D[pred1.pts3d - Camera1 åº§æ¨™ç³»]
    C --> E[pred2.pts3d_in_other_view - ä¹Ÿåœ¨ Camera1 åº§æ¨™ç³»]
    C --> F[desc1, desc2 - ç‰¹å¾µæè¿°å­]
    
    F --> G[fast_reciprocal_NNs - ç‰¹å¾µåŒ¹é…]
    G --> H[åŒ¹é…é»å° matches_1, matches_2]
    
    H --> I[å¾åŒ¹é…é»ç²å– 3D å°æ‡‰]
    D --> J[pts3d_1 åœ¨æ¨™è¨˜é»ä½ç½®]
    E --> K[pts3d_2 åœ¨åŒ¹é…é»ä½ç½®]
    
    J --> L[ä½¿ç”¨ PnP æˆ– Kabsch ä¼°è¨ˆç›¸å°å§¿æ…‹]
    K --> L
    
    L --> M[T_rel ç›¸å°å§¿æ…‹]
    M --> N[è¨ˆç®— P1 prime]
```

**æ–¹æ¡ˆ B çš„å•é¡Œï¼š**
1. `inference()` è¼¸å‡ºçš„ `pts3d` æ²’æœ‰ç¶“éå¤šè¦–è§’ä¸€è‡´æ€§å„ªåŒ–
2. éœ€è¦é¡å¤–æ­¥é©Ÿä¼°è¨ˆç›¸å°å§¿æ…‹
3. å°ºåº¦å¯èƒ½ä¸æº–ç¢º

---

## 2D vs 3D æ–¹æ¡ˆæ¯”è¼ƒ

| ç‰¹æ€§ | 2D æ–¹æ¡ˆ (Homography/Affine) | 3D æ–¹æ¡ˆ (sparse_ga) |
|------|------------------------------|---------------------|
| **é€Ÿåº¦** | å¿«ï¼ˆ~100msï¼‰ | æ…¢ï¼ˆ~2-5sï¼‰|
| **é©ç”¨å ´æ™¯** | è¿‘ä¼¼å¹³é¢ç‰©é«” | ä»»æ„ 3D å ´æ™¯ |
| **é€è¦–è®ŠåŒ–** | æœ‰é™åˆ¶ | è™•ç†è‰¯å¥½ |
| **è¼¸å‡º** | 2D åƒç´ åº§æ¨™ | 2D åƒç´  + 3D å§¿æ…‹ |
| **æ—‹è½‰ä¼°è¨ˆ** | ç„¡æ³•ç›´æ¥ç²å¾— | âœ… å¯ä»¥ç²å¾— |
| **æ·±åº¦è®ŠåŒ–** | ç„¡æ³•è™•ç† | âœ… å¯ä»¥ä¼°è¨ˆ |

---

## æ¨è–¦çš„æ•´åˆæ¶æ§‹

```mermaid
flowchart TD
    subgraph Input
        A[Template å½±åƒ]
        B[ROI å¤šé‚Šå½¢]
        C[æ¨™è¨˜é» P1]
        D[ç›®æ¨™åƒç´ ä½ç½® desired_pixel]
        E[Target å½±åƒ]
    end
    
    subgraph Processing
        F{é¸æ“‡æ–¹æ¡ˆ}
        F -->|å¿«é€Ÿ/å¹³é¢| G[2D æ–¹æ¡ˆ: Homography]
        F -->|ç²¾ç¢º/3D| H[3D æ–¹æ¡ˆ: sparse_ga]
        
        G --> I[P1_prime åƒç´ åº§æ¨™]
        H --> I
        H --> J[T_rel ç›¸å°å§¿æ…‹]
    end
    
    subgraph Control
        I --> K[pixel_error = P1_prime - desired_pixel]
        J --> L[rotation_error å¾ T_rel æå–]
        
        K --> M[ç¶“é©—æ˜ å°„ gain_translation]
        L --> N[ç¶“é©—æ˜ å°„ gain_rotation]
        
        M --> O[delta_position]
        N --> P[delta_rotation]
        
        O --> Q[æ©Ÿæ¢°è‡‚æŒ‡ä»¤]
        P --> Q
    end
    
    subgraph Iteration
        Q --> R[æ©Ÿæ¢°è‡‚ç§»å‹•]
        R --> S[ç­‰å¾…ç©©å®š]
        S --> T{æ”¶æ–‚?}
        T -->|å¦| E
        T -->|æ˜¯| U[å®Œæˆ]
    end
```

---

## æ§åˆ¶è¿´åœˆå½ä»£ç¢¼

```python
class VisualServoController:
    def __init__(self):
        self.transfer_3d = PolygonTargetTransfer3D()
        
        # ç¶“é©—å¢ç›Šï¼ˆéœ€è¦å¯¦é©—èª¿æ•´ï¼‰
        self.gain_translation = np.array([
            [0.0001, 0.0],      # pixel_error_x â†’ delta_arm_x
            [0.0, 0.0001],      # pixel_error_y â†’ delta_arm_y
        ])
        self.gain_rotation = 0.1  # rotation_error â†’ delta_arm_rotation
        
    def servo_loop(self, template_path, roi, target_point, desired_pixel, 
                   max_iterations=20, pixel_threshold=5.0):
        """
        è¿­ä»£è¦–è¦ºä¼ºæœæ§åˆ¶è¿´åœˆ
        """
        for iteration in range(max_iterations):
            # 1. æ‹æ”ç•¶å‰å½±åƒ
            target_path = self.capture_image()
            
            # 2. è¨ˆç®—æ¨™è¨˜é»ä½ç½®å’Œç›¸å°å§¿æ…‹
            result = self.transfer_3d.compute_polygon_transfer(
                template_path, target_path, roi, target_point
            )
            
            P1_prime = result['P1_prime']
            T_rel = result['T_rel']
            
            # 3. è¨ˆç®—åƒç´ èª¤å·®
            pixel_error = np.array(P1_prime) - np.array(desired_pixel)
            error_magnitude = np.linalg.norm(pixel_error)
            
            print(f"Iteration {iteration}: pixel_error = {pixel_error}, magnitude = {error_magnitude:.2f}")
            
            # 4. æª¢æŸ¥æ”¶æ–‚
            if error_magnitude < pixel_threshold:
                print(f"Converged! Final error: {error_magnitude:.2f} pixels")
                return True
            
            # 5. è¨ˆç®—æ©Ÿæ¢°è‡‚ä¿®æ­£é‡
            # å¹³ç§»éƒ¨åˆ†
            delta_translation = self.gain_translation @ pixel_error
            
            # æ—‹è½‰éƒ¨åˆ†ï¼ˆå¾ T_rel æå–ï¼‰
            rotation_matrix = T_rel[:3, :3]
            # ä½¿ç”¨ Rodrigues æˆ–å…¶ä»–æ–¹æ³•è½‰æ›ç‚ºè»¸è§’è¡¨ç¤º
            rotation_angle = rotation_matrix_to_angle(rotation_matrix)
            delta_rotation = self.gain_rotation * rotation_angle
            
            # 6. ç™¼é€æ©Ÿæ¢°è‡‚æŒ‡ä»¤
            self.robot.move_relative(
                delta_x=delta_translation[0],
                delta_y=delta_translation[1],
                delta_rotation=delta_rotation
            )
            
            # 7. ç­‰å¾…æ©Ÿæ¢°è‡‚ç©©å®š
            time.sleep(1.0)
        
        print(f"Max iterations reached. Final error: {error_magnitude:.2f} pixels")
        return False
```

---

## ç¸½çµ

### æ­£ç¢ºçš„ 3D Workflow æ ¸å¿ƒåŸå‰‡

1. **åº§æ¨™ç³»ä¸€è‡´æ€§**
   - ä½¿ç”¨ `sparse_global_alignment` æ™‚ï¼Œæ‰€æœ‰æ•¸æ“šï¼ˆposes, pts3dï¼‰éƒ½åœ¨åŒä¸€ä¸–ç•Œåº§æ¨™ç³»
   - âŒ ä¸è¦æ··ç”¨ `inference()` çš„ pts3d å’Œ `sparse_ga` çš„ poses

2. **æ•¸æ“šç²å–é †åº**
   ```python
   scene = sparse_global_alignment(...)
   
   # å¾åŒä¸€å€‹ scene ç²å–æ‰€æœ‰éœ€è¦çš„æ•¸æ“š
   poses = scene.get_im_poses()
   pts3d = scene.get_sparse_pts3d()
   focals = scene.get_focals()
   ```

3. **ç›¸å°å§¿æ…‹è¨ˆç®—**
   ```python
   cam2w_1 = poses[0]  # Template camera pose
   cam2w_2 = poses[1]  # Target camera pose
   
   # æ–¹æ³• 1: Camera2 åˆ° Camera1 çš„è®Šæ›
   T_rel = inv(cam2w_2) @ cam2w_1
   
   # æ–¹æ³• 2: ç›´æ¥è¨ˆç®—å·®ç•°ï¼ˆç”¨æ–¼æ©Ÿæ¢°è‡‚æ§åˆ¶ï¼‰
   position_diff = cam2w_2[:3, 3] - cam2w_1[:3, 3]
   rotation_diff = cam2w_2[:3, :3] @ cam2w_1[:3, :3].T
   ```

4. **è¿­ä»£æ”¶æ–‚**
   - æ–¹å‘æ˜¯å°çš„ï¼Œæ­¥ä¼ç”¨ç¶“é©—å¢ç›Šèª¿æ•´
   - å¤šæ¬¡è¿­ä»£ç›´åˆ° pixel_error < threshold
